"""
Content moderation service for Vivid AI platform
Implements multi-layer security and content safety measures
"""
import asyncio
import hashlib
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta

class ContentModerationService:
    """Service for content moderation and safety checks"""
    
    def __init__(self):
        # Content filtering patterns
        self.blocked_keywords = {
            'nsfw': [
                'explicit', 'nude', 'naked', 'sexual', 'pornographic', 'adult',
                'xxx', 'erotic', 'intimate', 'provocative'
            ],
            'violence': [
                'violent', 'blood', 'killing', 'murder', 'weapon', 'gun',
                'knife', 'assault', 'attack', 'harm', 'death'
            ],
            'hate': [
                'racist', 'hate', 'discrimination', 'slur', 'offensive',
                'bigot', 'supremacist', 'nazi', 'terrorist'
            ],
            'illegal': [
                'drug', 'cocaine', 'heroin', 'marijuana', 'illegal',
                'piracy', 'stolen', 'fraud', 'scam'
            ]
        }
        
        # Watermarking configuration
        self.watermark_config = {
            'enabled': True,
            'text': 'Generated by Vivid AI',
            'opacity': 0.3,
            'position': 'bottom-right',
            'blockchain_hash': True
        }
        
        # Rate limiting storage (in production, use Redis)
        self.rate_limits = {}
        self.user_violations = {}
    
    async def moderate_text_content(self, text: str, user_id: str) -> Dict[str, any]:
        """Moderate text content like prompts and descriptions"""
        
        # Check for blocked keywords
        violations = []
        confidence_scores = {}
        
        text_lower = text.lower()
        
        for category, keywords in self.blocked_keywords.items():
            category_violations = []
            for keyword in keywords:
                if keyword in text_lower:
                    category_violations.append(keyword)
            
            if category_violations:
                violations.append({
                    'category': category,
                    'keywords': category_violations,
                    'severity': self._get_severity(category)
                })
                confidence_scores[category] = min(len(category_violations) * 0.3, 1.0)
        
        # Check for suspicious patterns
        suspicious_patterns = self._check_suspicious_patterns(text)
        if suspicious_patterns:
            violations.extend(suspicious_patterns)
        
        # Calculate overall safety score
        safety_score = self._calculate_safety_score(violations, confidence_scores)
        
        # Log violation if any
        if violations:
            await self._log_violation(user_id, 'text', violations, text)
        
        return {
            'approved': safety_score >= 0.7,
            'safety_score': safety_score,
            'violations': violations,
            'confidence_scores': confidence_scores,
            'action': self._determine_action(safety_score, violations)
        }
    
    async def moderate_video_content(self, video_path: str, user_id: str) -> Dict[str, any]:
        """Moderate video content using AI analysis"""
        
        # Simulate video content analysis
        # In production, integrate with services like AWS Rekognition, Google Video Intelligence
        await asyncio.sleep(2)  # Simulate processing time
        
        # Mock analysis results
        analysis_results = {
            'nsfw_detection': {
                'confidence': 0.05,  # Low confidence = safe
                'labels': []
            },
            'violence_detection': {
                'confidence': 0.03,
                'labels': []
            },
            'face_detection': {
                'faces_detected': 2,
                'minors_detected': 0
            },
            'text_detection': {
                'text_found': ['Sample Text', 'Video Title'],
                'inappropriate_text': []
            },
            'audio_analysis': {
                'inappropriate_audio': False,
                'confidence': 0.02
            }
        }
        
        # Calculate overall safety score
        safety_factors = {
            'nsfw': 1.0 - analysis_results['nsfw_detection']['confidence'],
            'violence': 1.0 - analysis_results['violence_detection']['confidence'],
            'audio': 1.0 - analysis_results['audio_analysis']['confidence']
        }
        
        overall_safety = sum(safety_factors.values()) / len(safety_factors)
        
        # Check for violations
        violations = []
        if analysis_results['nsfw_detection']['confidence'] > 0.7:
            violations.append({
                'category': 'nsfw',
                'confidence': analysis_results['nsfw_detection']['confidence'],
                'severity': 'high'
            })
        
        if analysis_results['violence_detection']['confidence'] > 0.6:
            violations.append({
                'category': 'violence',
                'confidence': analysis_results['violence_detection']['confidence'],
                'severity': 'high'
            })
        
        # Log if violations found
        if violations:
            await self._log_violation(user_id, 'video', violations, video_path)
        
        return {
            'approved': overall_safety >= 0.8 and len(violations) == 0,
            'safety_score': overall_safety,
            'violations': violations,
            'analysis_results': analysis_results,
            'requires_review': 0.6 <= overall_safety < 0.8
        }
    
    async def apply_watermark(self, video_path: str, output_path: str) -> Dict[str, any]:
        """Apply invisible watermark to video"""
        
        if not self.watermark_config['enabled']:
            return {'watermarked': False, 'reason': 'Watermarking disabled'}
        
        # Simulate watermarking process
        await asyncio.sleep(1)
        
        # Generate unique watermark hash
        timestamp = datetime.utcnow().isoformat()
        content_hash = hashlib.sha256(f"{video_path}{timestamp}".encode()).hexdigest()
        
        watermark_data = {
            'generator': 'Vivid AI',
            'timestamp': timestamp,
            'content_hash': content_hash[:16],
            'version': '1.0'
        }
        
        # In production, embed this data invisibly in the video
        # For now, simulate the process
        
        return {
            'watermarked': True,
            'watermark_data': watermark_data,
            'blockchain_hash': content_hash if self.watermark_config['blockchain_hash'] else None,
            'output_path': output_path
        }
    
    async def check_rate_limits(self, user_id: str, action: str) -> Dict[str, any]:
        """Check if user has exceeded rate limits"""
        
        current_time = datetime.utcnow()
        hour_key = current_time.strftime('%Y-%m-%d-%H')
        day_key = current_time.strftime('%Y-%m-%d')
        
        # Rate limits by action
        limits = {
            'video_upload': {'hourly': 10, 'daily': 50},
            'video_process': {'hourly': 20, 'daily': 100},
            'api_request': {'hourly': 1000, 'daily': 10000}
        }
        
        if action not in limits:
            return {'allowed': True, 'message': 'No limits for this action'}
        
        # Initialize tracking if needed
        if user_id not in self.rate_limits:
            self.rate_limits[user_id] = {}
        
        user_limits = self.rate_limits[user_id]
        
        # Check hourly limit
        hourly_key = f"{action}_{hour_key}"
        hourly_count = user_limits.get(hourly_key, 0)
        
        # Check daily limit
        daily_key = f"{action}_{day_key}"
        daily_count = user_limits.get(daily_key, 0)
        
        hourly_limit = limits[action]['hourly']
        daily_limit = limits[action]['daily']
        
        if hourly_count >= hourly_limit:
            return {
                'allowed': False,
                'message': f'Hourly limit exceeded for {action}',
                'reset_time': (current_time + timedelta(hours=1)).isoformat(),
                'limit_type': 'hourly'
            }
        
        if daily_count >= daily_limit:
            return {
                'allowed': False,
                'message': f'Daily limit exceeded for {action}',
                'reset_time': (current_time + timedelta(days=1)).isoformat(),
                'limit_type': 'daily'
            }
        
        # Increment counters
        user_limits[hourly_key] = hourly_count + 1
        user_limits[daily_key] = daily_count + 1
        
        return {
            'allowed': True,
            'remaining_hourly': hourly_limit - (hourly_count + 1),
            'remaining_daily': daily_limit - (daily_count + 1)
        }
    
    async def detect_deepfake(self, video_path: str) -> Dict[str, any]:
        """Detect potential deepfake content"""
        
        # Simulate deepfake detection
        await asyncio.sleep(3)
        
        # Mock deepfake analysis
        analysis = {
            'is_deepfake': False,
            'confidence': 0.15,  # Low confidence = likely real
            'face_consistency': 0.92,
            'temporal_consistency': 0.89,
            'artifact_detection': {
                'blending_artifacts': 0.1,
                'compression_artifacts': 0.2,
                'temporal_artifacts': 0.05
            }
        }
        
        # Determine if content should be flagged
        deepfake_threshold = 0.7
        flagged = analysis['confidence'] > deepfake_threshold
        
        return {
            'flagged': flagged,
            'confidence': analysis['confidence'],
            'analysis': analysis,
            'action': 'block' if flagged else 'approve'
        }
    
    def _check_suspicious_patterns(self, text: str) -> List[Dict[str, any]]:
        """Check for suspicious patterns in text"""
        
        violations = []
        
        # Check for excessive special characters (potential spam)
        special_char_ratio = len(re.findall(r'[^a-zA-Z0-9\s]', text)) / max(len(text), 1)
        if special_char_ratio > 0.3:
            violations.append({
                'category': 'spam',
                'pattern': 'excessive_special_characters',
                'severity': 'medium'
            })
        
        # Check for repeated characters (potential spam)
        if re.search(r'(.)\1{4,}', text):
            violations.append({
                'category': 'spam',
                'pattern': 'repeated_characters',
                'severity': 'low'
            })
        
        # Check for suspicious URLs
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        urls = re.findall(url_pattern, text)
        if urls:
            violations.append({
                'category': 'suspicious_links',
                'pattern': 'urls_detected',
                'severity': 'medium',
                'urls': urls
            })
        
        return violations
    
    def _get_severity(self, category: str) -> str:
        """Get severity level for violation category"""
        severity_map = {
            'nsfw': 'high',
            'violence': 'high',
            'hate': 'high',
            'illegal': 'high',
            'spam': 'medium',
            'suspicious_links': 'medium'
        }
        return severity_map.get(category, 'low')
    
    def _calculate_safety_score(self, violations: List[Dict], confidence_scores: Dict) -> float:
        """Calculate overall safety score"""
        
        if not violations:
            return 1.0
        
        # Base score
        base_score = 1.0
        
        # Deduct points based on violations
        for violation in violations:
            severity = violation.get('severity', 'low')
            deduction = {
                'high': 0.4,
                'medium': 0.2,
                'low': 0.1
            }.get(severity, 0.1)
            
            base_score -= deduction
        
        # Apply confidence scores
        for category, confidence in confidence_scores.items():
            base_score -= confidence * 0.3
        
        return max(0.0, base_score)
    
    def _determine_action(self, safety_score: float, violations: List[Dict]) -> str:
        """Determine what action to take based on safety score"""
        
        high_severity_violations = [v for v in violations if v.get('severity') == 'high']
        
        if high_severity_violations:
            return 'block'
        elif safety_score < 0.5:
            return 'block'
        elif safety_score < 0.7:
            return 'review'
        else:
            return 'approve'
    
    async def _log_violation(self, user_id: str, content_type: str, violations: List[Dict], content: str):
        """Log content violations for monitoring"""
        
        # Track user violations
        if user_id not in self.user_violations:
            self.user_violations[user_id] = []
        
        violation_log = {
            'timestamp': datetime.utcnow().isoformat(),
            'content_type': content_type,
            'violations': violations,
            'content_preview': content[:100] if isinstance(content, str) else str(content)[:100]
        }
        
        self.user_violations[user_id].append(violation_log)
        
        # In production, send to logging service
        print(f"VIOLATION LOG - User: {user_id}, Type: {content_type}, Violations: {len(violations)}")
    
    async def get_user_safety_report(self, user_id: str) -> Dict[str, any]:
        """Get safety report for a user"""
        
        violations = self.user_violations.get(user_id, [])
        
        if not violations:
            return {
                'user_id': user_id,
                'total_violations': 0,
                'risk_level': 'low',
                'account_status': 'good_standing'
            }
        
        # Analyze violation patterns
        recent_violations = [v for v in violations 
                           if datetime.fromisoformat(v['timestamp']) > datetime.utcnow() - timedelta(days=30)]
        
        violation_categories = {}
        for violation in recent_violations:
            for v in violation['violations']:
                category = v.get('category', 'unknown')
                violation_categories[category] = violation_categories.get(category, 0) + 1
        
        # Determine risk level
        total_recent = len(recent_violations)
        if total_recent >= 10:
            risk_level = 'high'
            account_status = 'suspended'
        elif total_recent >= 5:
            risk_level = 'medium'
            account_status = 'warning'
        else:
            risk_level = 'low'
            account_status = 'good_standing'
        
        return {
            'user_id': user_id,
            'total_violations': len(violations),
            'recent_violations': total_recent,
            'violation_categories': violation_categories,
            'risk_level': risk_level,
            'account_status': account_status,
            'last_violation': violations[-1]['timestamp'] if violations else None
        }


# Global service instance
content_moderation = ContentModerationService()